{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 1238 restaurants closed on first inspection.\n",
      "\n",
      "There were 547 remained closed on subsequent inspection.\n",
      "\n",
      "There were 810 cleared and opened by subsequent inspection.\n",
      "\n",
      "There were 365 reopened but will need another inspection.\n",
      "\n",
      "1238 + 547 + 810 + 365 +  = 2960\n",
      "\n",
      "There were 2960 that had some word like 'emergency'.\n",
      "\n",
      "So it looks like we got them all.\n"
     ]
    }
   ],
   "source": [
    "# Code to analyze emergency closure data\n",
    "\n",
    "\"\"\"\n",
    "Here are codes used by state inspectors when they determine\n",
    "a restaurant should be closed temporarily. This is taken from\n",
    "http://www.myfloridalicense.com/DBPR/hotels-restaurants/inspections/inspection-dispositions/\n",
    "\n",
    "\n",
    "Facility Temporarily Closed:\n",
    "Operations ordered stopped until violations are corrected\n",
    "The inspector recommended closing the facility immediately\n",
    "after finding conditions that may endanger the health and\n",
    "safety of the public.\n",
    "\n",
    "Dispositions included in this result are:\n",
    "\n",
    "Emergency order recommended – Conditions have been found\n",
    "that endanger the health and safety of the public requiring\n",
    "immediate closure of the establishment.\n",
    "\n",
    "Administrative determination recommended – The establishment\n",
    "is operating without a license and action is being taken\n",
    "to ensure proper licensing is completed.\n",
    "\n",
    "Emergency Order Callback Not Complied – Corrections to violations\n",
    "that resulted in an emergency order were not completed\n",
    "at the time of inspection. Violations may not be noted\n",
    "again on these inspection reports.\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# Set up connection to database.\n",
    "\n",
    "imacpath = \"/Users/rayd/workspace/flinsp/datafiles/\"\n",
    "airpath = \"/Users/Doug/workspace/flinsp/datafiles/\"\n",
    "dbfile = \"rinspect18.sqlite\"\n",
    "dbpath = imacpath + dbfile\n",
    "conn = sqlite3.connect(dbpath)\n",
    "\n",
    "# Make a list of visitid numbers for restaurants shut down\n",
    "# by inspectors.\n",
    "\n",
    "conn.row_factory = lambda cursor, row: row[0]\n",
    "c = conn.cursor()\n",
    "\n",
    "# on initial inspection\n",
    "vids = c.execute(\"SELECT visitid FROM fdinsp WHERE inspdispos = 'Emergency order recommended'\").fetchall()\n",
    "\n",
    "# remained shut on subsequent inspection\n",
    "vids2 = c.execute(\"SELECT visitid FROM fdinsp WHERE inspdispos = 'Emergency Order Callback Not Complied'\").fetchall()\n",
    "\n",
    "# Make pandas dataframe for both sets\n",
    "\n",
    "conn = sqlite3.connect(dbpath)\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM fdinsp WHERE inspdispos = 'Emergency order recommended';\", conn)\n",
    "df2 = pd.read_sql_query(\"SELECT * FROM fdinsp WHERE inspdispos = 'Emergency Order Callback Not Complied';\", conn)\n",
    "\n",
    "# Test to see if our dataset is complete\n",
    "\n",
    "count1 = df.shape\n",
    "count2 = df2.shape\n",
    "print(\"There were \" + str(count1[0]) + \" restaurants closed on first inspection.\")\n",
    "print(\"\\nThere were \" + str(count2[0]) + \" remained closed on subsequent inspection.\")\n",
    "\n",
    "# Contains 'Emergency' but means reopened\n",
    "df_test1 = pd.read_sql_query(\n",
    "    \"SELECT * FROM fdinsp WHERE inspdispos = 'Emergency Order Callback Complied';\", conn\n",
    "    )\n",
    "count3 = df_test1.shape\n",
    "print(\"\\nThere were \" + str(count3[0]) + \" cleared and opened by subsequent inspection.\")\n",
    "\n",
    "# Contains 'Emergency' but also means reopened\n",
    "df_test2 = pd.read_sql_query(\n",
    "    \"SELECT * FROM fdinsp WHERE inspdispos = 'Emergency Order Callback Time Extension';\", conn\n",
    "    )\n",
    "count4 = df_test2.shape\n",
    "print(\"\\nThere were \" + str(count4[0]) + \" reopened but will need another inspection.\")\n",
    "\n",
    "print(\"\\n\" +\n",
    "    str(count1[0]) + \" + \" +\n",
    "    str(count2[0]) + \" + \" +\n",
    "    str(count3[0]) + \" + \" +\n",
    "    str(count4[0]) + \" + \" +\n",
    "    \" = \" + str(count1[0] + count2[0] + count3[0] + count4[0])\n",
    "     )\n",
    "\n",
    "# Contains something like 'Emergency' but are there some where spelling or capitalization shifts?\n",
    "df_test3 = pd.read_sql_query(\"SELECT * FROM fdinsp WHERE inspdispos LIKE '%mergency%';\", conn)\n",
    "count5 = df_test3.shape\n",
    "print(\"\\nThere were \" + str(count5[0]) + \" that had some word like 'emergency'.\")\n",
    "\n",
    "print(\"\\nSo it looks like we got them all.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of dictionaries with detailed inspection reports\n",
    "# that led to closures\n",
    "\n",
    "def dict_factory(cursor, row):\n",
    "    dvio = {}\n",
    "    for idx, col in enumerate(cursor.description):\n",
    "        dvio[col[0]] = row[idx]\n",
    "    return dvio\n",
    "\n",
    "lvio = []\n",
    "lvio2 = []\n",
    "\n",
    "for vid in vids:\n",
    "    con = sqlite3.connect(dbpath)\n",
    "    con.row_factory = dict_factory\n",
    "    cur = con.cursor()\n",
    "    cur.execute(f\"SELECT * FROM violations WHERE visitid = {vid}\")\n",
    "    lvio.extend(cur.fetchall())\n",
    "    con.close()\n",
    "\n",
    "for vid2 in vids2:\n",
    "    con = sqlite3.connect(dbpath)\n",
    "    con.row_factory = dict_factory\n",
    "    cur = con.cursor()\n",
    "    cur.execute(f\"SELECT * FROM violations WHERE visitid = {vid2}\")\n",
    "    lvio2.extend(cur.fetchall())\n",
    "    con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>details_id</th>\n",
       "      <th>id</th>\n",
       "      <th>obs</th>\n",
       "      <th>visitid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>violation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35A-05-4</th>\n",
       "      <td>541</td>\n",
       "      <td>541</td>\n",
       "      <td>541</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35A-03-4</th>\n",
       "      <td>466</td>\n",
       "      <td>466</td>\n",
       "      <td>466</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03A-02-4</th>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35A-04-4</th>\n",
       "      <td>383</td>\n",
       "      <td>383</td>\n",
       "      <td>383</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35A-02-5</th>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31B-02-4</th>\n",
       "      <td>239</td>\n",
       "      <td>239</td>\n",
       "      <td>239</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01B-02-4</th>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31A-03-4</th>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02C-02-4</th>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53B-01-5</th>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           details_id   id  obs  visitid\n",
       "violation                               \n",
       "35A-05-4          541  541  541      541\n",
       "35A-03-4          466  466  466      466\n",
       "03A-02-4          410  410  410      410\n",
       "35A-04-4          383  383  383      383\n",
       "35A-02-5          277  277  277      277\n",
       "31B-02-4          239  239  239      239\n",
       "01B-02-4          203  203  203      203\n",
       "31A-03-4          197  197  197      197\n",
       "02C-02-4          179  179  179      179\n",
       "53B-01-5          176  176  176      176"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make dataframes with violation details\n",
    "\n",
    "df3 = pd.DataFrame(lvio) # Closed on initial inspection\n",
    "df4 = pd.DataFrame(lvio2) # Remained closed after follow-up\n",
    "\n",
    "# What was the most common violation in a closure inspection?\n",
    "\n",
    "df3g = df3.groupby('violation').count().sort_values(by=['visitid'], axis=0, ascending=False)\n",
    "\n",
    "# What were the most common violations in a closure inspection?\n",
    "\n",
    "df3.groupby('violation').count().sort_values(\n",
    "    by=['visitid'], axis=0, ascending=False\n",
    "    ).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the df of inspections using the summary reports that contained\n",
      "the words 'Emergency order recommended', we saw 1238 initial closures.\n",
      "But in checking the detailed reports, we find 303 with 'Stop sale'\n",
      "Can you have a closure without a 'Stop sale'?\n",
      "Answers: Yes. 'Stop sale' refers to food item, maybe bad temp, not to the restaurant generally.\n"
     ]
    }
   ],
   "source": [
    "df_stopsale = df3[df3.obs.str.contains(\"Stop Sale issued\")==True]\n",
    "df_stopsale.shape\n",
    "print(\"In the df of inspections using the summary reports that contained\")\n",
    "print(\"the words 'Emergency order recommended', we saw \" + str(count1[0]) + \" initial closures.\" )\n",
    "print(\"But in checking the detailed reports, we find \" + str(df_stopsale.shape[0]) + \" with 'Stop sale'\")\n",
    "print(\"Can you have a closure without a 'Stop sale'?\")\n",
    "print(\"Answers: Yes. 'Stop sale' refers to food item, maybe bad temp, not to the restaurant generally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This approach finds whether, if a violation is present, it always\n",
      "results in a closure order:\n",
      "\n",
      "The number of closures on initial inspection = 16625\n",
      "\n",
      "The number of closures, despite this being found:\n",
      "'Service animals' = 16604\n",
      "'Live, small flying insects in food service area' = 11548\n",
      "'Dead roaches on premesis' = 8358\n",
      "'Rodent activity present as evidenced by droppings' = 10255\n",
      "'Live roaches found' = 7557\n",
      "'Accumulation of dead or trapped pests' = 15848\n",
      "'Small flying insects in bar, kitchen, dumster, prep area' = 16006\n",
      "'Presence of insects, rodents or other pests' = 15831\n",
      "'Rodent rub marks present' = 0\n",
      "'Dead rodent present' = 16250\n",
      "'Rodent burrow or nesting materials present' = 16118\n",
      "'Rodent droppings present' = 15224\n",
      "'Potentially hazardous  cold food held at greater than 41 degrees' = 8477\n",
      "\n",
      "So, if 'Rodent rub marks present', the doors are closed!\n"
     ]
    }
   ],
   "source": [
    "# Can we find which violations resulted in the most closures?\n",
    "\n",
    "# Violation codes and descriptions\n",
    "a = '35A-01-4' # Intermediate: Service animals\n",
    "b = '35A-02-5' # High priority: Live, small flying insects in food service area\n",
    "c = '35A-03-4' # Basic: Dead roaches on premesis\n",
    "d = '35A-04-4' # High priority: Rodent activity present as evidenced by droppings\n",
    "e = '35A-05-4' # High priority: Live roaches found\n",
    "f = '35A-06-4' # Basic: Accumulation of dead or trapped pests\n",
    "g = '35A-07-4' # High priority: Small flying insects in bar, kitchen, dumster, prep area\n",
    "h = '35A-09-4' # High priority: Presence of insects, rodents or other pests\n",
    "i = '35A-18-4' # High priority: Rodent rub marks present\n",
    "j = '35A-20-4' # Basic: Dead rodent present\n",
    "k = '35A-21-4' # High priority: Rodent burrow or nesting materials present\n",
    "l = '35A-23-4' # High priority: Rodent droppings present\n",
    "m = '03A-02-4' # High priority: Potentially hazardous  cold food held at greater than 41 degrees\n",
    "\n",
    "# Are there any closures that don't involve 35A-0*'?\n",
    "df3z = df3\n",
    "df3za = df3.groupby('visitid').filter(lambda x: all(a != i for i in x['violation']))\n",
    "df3zb = df3z.groupby('visitid').filter(lambda x: all(b != i for i in x['violation']))\n",
    "df3zc = df3z.groupby('visitid').filter(lambda x: all(c != i for i in x['violation']))\n",
    "df3zd = df3z.groupby('visitid').filter(lambda x: all(d != i for i in x['violation']))\n",
    "df3ze = df3z.groupby('visitid').filter(lambda x: all(e != i for i in x['violation']))\n",
    "df3zf = df3z.groupby('visitid').filter(lambda x: all(f != i for i in x['violation']))\n",
    "df3zg = df3z.groupby('visitid').filter(lambda x: all(g != i for i in x['violation']))\n",
    "df3zh = df3z.groupby('visitid').filter(lambda x: all(h != i for i in x['violation']))\n",
    "df3zi = df3z.groupby('visitid').filter(lambda x: all(i != i for i in x['violation']))\n",
    "df3zj = df3z.groupby('visitid').filter(lambda x: all(j != i for i in x['violation']))\n",
    "df3zk = df3z.groupby('visitid').filter(lambda x: all(k != i for i in x['violation']))\n",
    "df3zl = df3z.groupby('visitid').filter(lambda x: all(l != i for i in x['violation']))\n",
    "df3zm = df3z.groupby('visitid').filter(lambda x: all(m != i for i in x['violation']))\n",
    "\n",
    "#On checking database directly, this doesn't seem to work ....\n",
    "print(\"This approach finds whether, if a violation is present, it always\")\n",
    "print(\"results in a closure order:\")\n",
    "print(\"\\nThe number of closures on initial inspection = \" + str(df3z.shape[0]))\n",
    "print(\"\\nThe number of closures, despite this being found:\")\n",
    "print(\"'Service animals' = \" + str(df3za.shape[0]))\n",
    "print(\"'Live, small flying insects in food service area' = \" + str(df3zb.shape[0]))\n",
    "print(\"'Dead roaches on premesis' = \" + str(df3zc.shape[0]))\n",
    "print(\"'Rodent activity present as evidenced by droppings' = \" + str(df3zd.shape[0]))\n",
    "print(\"'Live roaches found' = \" + str(df3ze.shape[0]))\n",
    "print(\"'Accumulation of dead or trapped pests' = \" + str(df3zf.shape[0]))\n",
    "print(\"'Small flying insects in bar, kitchen, dumster, prep area' = \" + str(df3zg.shape[0]))\n",
    "print(\"'Presence of insects, rodents or other pests' = \" + str(df3zh.shape[0]))\n",
    "print(\"'Rodent rub marks present' = \" + str(df3zi.shape[0]))\n",
    "print(\"'Dead rodent present' = \" + str(df3zj.shape[0]))\n",
    "print(\"'Rodent burrow or nesting materials present' = \" + str(df3zk.shape[0]))\n",
    "print(\"'Rodent droppings present' = \" + str(df3zl.shape[0]))\n",
    "print(\"'Potentially hazardous  cold food held at greater than 41 degrees' = \" + str(df3zm.shape[0]))\n",
    "print(\"\\nSo, if 'Rodent rub marks present', the doors are closed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              closures\n",
      "county                \n",
      "Alachua             14\n",
      "Baker                2\n",
      "Bay                 19\n",
      "Bradford             2\n",
      "Brevard             48\n",
      "Broward            114\n",
      "Charlotte            5\n",
      "Citrus              10\n",
      "Clay                 4\n",
      "Collier             17\n",
      "Columbia             7\n",
      "Dade               145\n",
      "Dixie                1\n",
      "Duval              108\n",
      "Escambia            19\n",
      "Flagler              2\n",
      "Franklin             2\n",
      "Gadsden              6\n",
      "Gilchrist            2\n",
      "Glades               1\n",
      "Hardee               1\n",
      "Hendry               1\n",
      "Hernando            23\n",
      "Highlands            7\n",
      "Hillsborough        88\n",
      "Indian River         4\n",
      "Jackson              5\n",
      "Jefferson            1\n",
      "Lake                16\n",
      "Lee                 28\n",
      "Leon                32\n",
      "Levy                 1\n",
      "Madison              1\n",
      "Manatee             27\n",
      "Marion              44\n",
      "Martin               1\n",
      "Monroe               7\n",
      "Okaloosa             9\n",
      "Okeechobee           3\n",
      "Orange              75\n",
      "Osceola              4\n",
      "Palm Beach          73\n",
      "Pasco               27\n",
      "Pinellas            96\n",
      "Polk                34\n",
      "Putnam               3\n",
      "Santa Rosa           8\n",
      "Sarasota            16\n",
      "Seminole            10\n",
      "St. Johns           22\n",
      "St. Lucie           12\n",
      "Sumter               3\n",
      "Suwannee             3\n",
      "Volusia             20\n",
      "Wakulla              3\n",
      "Walton               2\n",
      "\n",
      "Did any counties not have closure orders in FY2018-19?\n",
      "\n",
      "These are not included: Calhoun, DeSoto, Gulf, Hamilton, Holmes, Lafayette, Liberty, Miami-Dade, Nassau, Taylor, Union, Washington\n",
      "\n",
      "But Miami-Dade listed simply as Dade in our data frame.\n"
     ]
    }
   ],
   "source": [
    "# Which counties had the most closures?\n",
    "# Calculated as closers per licensed restaurant\n",
    "\n",
    "# Count closures per county\n",
    "dfc = df.groupby('county').count()\n",
    "dfc = dfc.licnum.reset_index()\n",
    "dfc = dfc.rename(columns={'county' : 'county', 'licnum' : 'closures'})\n",
    "dfc = dfc.set_index('county')\n",
    "print(dfc)\n",
    "\n",
    "# Which counties are included in closures\n",
    "co_inc = list(df.groupby(['county']).groups.keys())\n",
    "\n",
    "#List of all Florida counties\n",
    "with open('outfiles/counties.txt', 'r') as f:\n",
    "    fl_counties = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "def diff(co_inc, fl_counties):\n",
    "    co_dif = [i for i in co_inc + fl_counties if i not in co_inc]\n",
    "    return co_dif\n",
    "\n",
    "missing_counties = diff(co_inc, fl_counties)\n",
    "\n",
    "print(\"\\nDid any counties not have closure orders in FY2018-19?\")\n",
    "print(\"\\nThese are not included: \" + str(', '.join(missing_counties)))\n",
    "print(\"\\nBut Miami-Dade listed simply as Dade in our data frame.\")\n",
    "\n",
    "missing = list(missing_counties)\n",
    "missing.remove('Miami-Dade')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'countycount.csv' does not exist: b'countycount.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8c897a455bf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read in csv of licensed restaurants per county\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_cntylic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'countycount.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_cntylic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cntylic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_cntylic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cntylic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf_cntylic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'co_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# drop missing counties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_cntylic\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf_cntylic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"lic_count\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"licenses\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"co_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"county\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fdinsp/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fdinsp/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fdinsp/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fdinsp/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fdinsp/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'countycount.csv' does not exist: b'countycount.csv'"
     ]
    }
   ],
   "source": [
    "# Read in csv of licensed restaurants per county\n",
    "df_cntylic = pd.read_csv('countycount.csv')\n",
    "df_cntylic = df_cntylic.drop(['Unnamed: 0'], axis=1)\n",
    "df_cntylic = df_cntylic[~df_cntylic['co_name'].isin(missing)] # drop missing counties\n",
    "df_cntylic= df_cntylic.rename(columns={\"lic_count\": \"licenses\", \"co_name\": \"county\"})\n",
    "df_cntylic = df_cntylic.set_index('county')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closures per license\n",
    "dfc = df_cntylic.join(dfc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['ratio'] = dfc.closures / dfc.licenses\n",
    "dfc['percent'] = dfc.ratio * 100\n",
    "dfc = dfc.sort_values(by=['ratio'])\n",
    "most_closed = dfc.sort_values(by=['ratio'], ascending=False).head(10)\n",
    "least_closed  = dfc.sort_values(by=['ratio'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_closed.head(20)\n",
    "print(\"Counties as outliers for the most closures:\")\n",
    "print(most_closed.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_closed.head(20)\n",
    "print(\"Counties as outliers for the least closures:\")\n",
    "print(least_closed.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List restaurants by how many closures they had this year.\n",
    "\n",
    "# Drop unneded columns\n",
    "dropcols = ['librow', 'inspnum', 'insptype', 'inspdispos', 'inspdate', 'totalvio', 'highvio', 'licid', 'visitid', 'time_now', 'time_posted']\n",
    "df_restaurants = df.drop(dropcols, axis=1)\n",
    "df_restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count repeated closures per restaurant, listed by license number\n",
    "dupes = df_restaurants.pivot_table(index=['licnum'],aggfunc='size')\n",
    "dupes = dupes.reset_index()\n",
    "\n",
    "# Drop repeated rows so we can match with the dupes\n",
    "df_repeats = df_restaurants[df_restaurants.duplicated()]\n",
    "\n",
    "# Merge dataframes to get df with count of repeats\n",
    "dupes_count = pd.merge(df_repeats, dupes, on='licnum')\n",
    "dupes_count = dupes_count.sort_values(by=[0], ascending=False)\n",
    "dupes_count = dupes_count[dupes_count.duplicated()]\n",
    "dupes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO RUN EACH TIME\n",
    "\n",
    "# Write restaurants with initial closures to csv \n",
    "df_restaurants = df_restaurants.drop_duplicates()\n",
    "df_restaurants.to_csv('outfiles/closedrestaurants.csv', index=False)\n",
    "\n",
    "# Write csv files for violation details\n",
    "\n",
    "keys = lvio[0].keys\n",
    "with open('outfiles/closurevios.csv', 'w', newline='') as output_file:\n",
    "    fc = csv.DictWriter(output_file,\n",
    "                        fieldnames=lvio[0].keys()\n",
    "                       )\n",
    "    fc.writeheader()\n",
    "    fc.writerows(lvio)\n",
    "\n",
    "keys = lvio2[0].keys\n",
    "with open('outfiles/closurevios2.csv', 'w', newline='') as output_file:\n",
    "    fc = csv.DictWriter(output_file,\n",
    "                        fieldnames=lvio2[0].keys()\n",
    "                       )\n",
    "    fc.writeheader()\n",
    "    fc.writerows(lvio2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of all violations involved in initial closures\n",
    "clist = df3.violation.values.tolist()\n",
    "clist = list(dict.fromkeys(clist))\n",
    "clist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
